{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections.abc import Iterable, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction(ABC):\n",
    "    \"\"\"Abstract class for cost functions\"\"\"\n",
    "    @abstractmethod\n",
    "    def functional(self, y_true: 'np.ndarray[float]', y_pred: 'np.ndarray[float]') -> float:\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_array(y: Iterable[float]) -> 'np.ndarray[float]':\n",
    "        return np.fromiter(y, float)\n",
    "\n",
    "    def make_scorer(self) -> Callable:\n",
    "        return sm.make_scorer(self.functional, greater_is_better=False)\n",
    "\n",
    "    def __call__(self, y_true: Iterable[float], y_pred: Iterable[float]) -> float:\n",
    "        y_pred_array = self._to_array(y_pred)\n",
    "        y_true_array = self._to_array(y_true)\n",
    "            \n",
    "        return self.functional(y_true_array, y_pred_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationCostFunction(CostFunction):\n",
    "    def __init__(self, metric_opt_val_map: Iterable[tuple[str, float, float]], metric_class_map: dict[str, str]={}, proba_threshold: float = 0.5):\n",
    "        \"\"\"Defines cost functional for optimization of multiple metrics. \n",
    "        Since this is defined as a loss function, cross validation returns the negative of the score [1].\n",
    "\n",
    "        Args:\n",
    "            metric_opt_val_map (Iterable[tuple[str, float, float]]): Iterable of tuples of the form (metric_name, optimal_value, weight).\n",
    "            metric_class_map (dict[str, str], optional): Dictionary mapping metric to class or probability of the form {'metric': 'class' or 'proba'}. Defaults to {}.\n",
    "            proba_threshold (float, optional): Probability threshold used to convert probabilities into classes. Defaults to 0.5.\n",
    "            \n",
    "        References:\n",
    "            [1] https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "            \n",
    "        Example:\n",
    "            >>> y_true = [0, 0, 0, 1, 1]\n",
    "            >>> y_pred = [0.46, 0.6, 0.29, 0.25, 0.012]\n",
    "            >>> threshold = 0.5\n",
    "            >>> score_opt_val_map = [(\"f1_score\", 1, 1),\n",
    "            ... (\"log_loss\", 0, 1),\n",
    "            ... (\"roc_auc_score\", 1, 1)]\n",
    "            >>> cf = ClassificationCostFunction(score_opt_val_map)\n",
    "            >>> round(cf.functional(y_true, y_pred))\n",
    "            21\n",
    "            >>> score_opt_val_map = [(\"f1_score\", 1, 1)]\n",
    "            >>> cf = ClassificationCostFunction(score_opt_val_map)\n",
    "            >>> X, y = make_classification()\n",
    "            >>> model = LogisticRegression()\n",
    "            >>> model.fit(X, y)\n",
    "            >>> y_proba = model.predict_proba(X)[:, 1]\n",
    "            >>> class_output = cf(y, y_proba)\n",
    "            >>> scorer = getattr(sm, \"f1_score\")\n",
    "            >>> y_pred = np.where(y_proba > 0.5, 1, 0)\n",
    "            >>> scorer_output = (float(scorer(y, y_pred)) - 1.0)**2\n",
    "            >>> np.isclose(class_output, scorer_output)\n",
    "            True\n",
    "        \"\"\"\n",
    "        self.metric_opt_val_map = metric_opt_val_map\n",
    "        self.proba_threshold = proba_threshold\n",
    "        self.metric_class_map = metric_class_map or {\n",
    "            \"accuracy_score\": \"class\",\n",
    "            \"f1_score\": \"class\",\n",
    "            \"log_loss\": \"class\",\n",
    "            \"precision_score\": \"class\",\n",
    "            \"recall_score\": \"class\",\n",
    "            \"roc_auc_score\": \"proba\"\n",
    "        }\n",
    "        \n",
    "    def _to_class(self, array: 'np.ndarray[float]', metric: str) -> 'np.ndarray[float]':\n",
    "        return np.where(array > self.proba_threshold, 1, 0) if self.metric_class_map[metric] == \"class\" else array\n",
    "    \n",
    "    \n",
    "    def functional(self, y_true: 'np.ndarray[float]', y_pred: 'np.ndarray[float]') -> float:\n",
    "        \n",
    "        cost = 0\n",
    "        for (score_name, opt_val, weight) in self.metric_opt_val_map:\n",
    "            scorer = getattr(sm, score_name)\n",
    "            \n",
    "            y_hat = self._to_class(y_pred, score_name)\n",
    "                \n",
    "            cost += weight * (scorer(y_true, y_hat) - opt_val) ** 2\n",
    "            \n",
    "        return cost\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_opt_val_map = [\n",
    "        (\"f1_score\", 1, 1),\n",
    "]\n",
    "cf = ClassificationCostFunction(score_opt_val_map)\n",
    "X, y = make_classification()\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "y_proba = model.predict_proba(X)[:, 1]\n",
    "class_output = cf(y, y_proba)\n",
    "\n",
    "scorer = getattr(sm, \"f1_score\")\n",
    "y_pred = np.where(y_proba > 0.5, 1, 0)\n",
    "scorer_output = (float(scorer(y, y_pred)) - 1.0)**2\n",
    "\n",
    "assert np.isclose(class_output, scorer_output), f\"{class_output} != {scorer_output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036147</td>\n",
       "      <td>0.043930</td>\n",
       "      <td>0.028570</td>\n",
       "      <td>0.016946</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5}</td>\n",
       "      <td>-26.950154</td>\n",
       "      <td>-11.979547</td>\n",
       "      <td>-3.000088</td>\n",
       "      <td>-27.007001</td>\n",
       "      <td>-11.985865</td>\n",
       "      <td>-16.184531</td>\n",
       "      <td>9.403879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008821</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>-47.904298</td>\n",
       "      <td>-11.979547</td>\n",
       "      <td>-3.000088</td>\n",
       "      <td>-27.007001</td>\n",
       "      <td>-11.985865</td>\n",
       "      <td>-20.375360</td>\n",
       "      <td>15.777164</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.036147      0.043930         0.028570        0.016946     0.5   \n",
       "1       0.008821      0.002099         0.008231        0.001511       1   \n",
       "\n",
       "       params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.5}         -26.950154         -11.979547          -3.000088   \n",
       "1    {'C': 1}         -47.904298         -11.979547          -3.000088   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0         -27.007001         -11.985865       -16.184531        9.403879   \n",
       "1         -27.007001         -11.985865       -20.375360       15.777164   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                2  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_opt_val_map = [\n",
    "        (\"accuracy_score\", 1, 1),\n",
    "        (\"f1_score\", 1, 1),\n",
    "        (\"log_loss\", 0, 1),\n",
    "        (\"precision_score\", 1, 1),\n",
    "        (\"recall_score\", 1, 1),\n",
    "        (\"roc_auc_score\", 1, 1),\n",
    "]\n",
    "\n",
    "param_grid = {\"C\": [0.5, 1]}\n",
    "\n",
    "scorer = ClassificationCostFunction(score_opt_val_map, proba_threshold=0.5)\n",
    "cv = GridSearchCV(LogisticRegression(), param_grid, scoring=scorer.make_scorer())\n",
    "\n",
    "X, y = make_classification()\n",
    "cv.fit(X, y)\n",
    "pd.DataFrame.from_dict(cv.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
